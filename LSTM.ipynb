{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "thousand-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout,TimeDistributed, Conv1D, MaxPooling1D, Flatten, Reshape\n",
    "from keras.regularizers import l2\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-wrestling",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "separate-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"datasets/all_features.csv\")\n",
    "raw = raw.drop(columns = [\"SOFR\",\"SOFRVOL\",\"EFFRVOL\",\"OBFR\",\"OBFRVOL\",\"EFFRVOL_DIFF\"\n",
    "                          ,\"OBFR_DIFF\",\"OBFRVOL_DIFF\",\"SP500_UNNORM\", \"SOFR_DIFF\", \"SOFRVOL_DIFF\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "express-warehouse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = raw[raw[\"date\"] < \"2018-00-00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "personalized-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = raw[raw[\"date\"] > \"2018-00-00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "transparent-february",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1711, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ethical-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"datasets/train.csv\")\n",
    "test.to_csv(\"datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "polish-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns = [\"SP500_NORM\", \"date\"]).values\n",
    "y_train = train[\"SP500_NORM\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "swiss-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(columns = [\"SP500_NORM\", \"date\"]).values\n",
    "y_test = test[\"SP500_NORM\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "temporal-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(X, y, n_steps):\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    for i in range(1, len(X)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps - 1\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(X)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = np.hstack([X[i:end_ix+1], y[i-1:end_ix].reshape((-1,1))]), y[end_ix]\n",
    "        new_X.append(seq_x)\n",
    "        new_y.append(seq_y)\n",
    "    return array(new_X), array(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "apart-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_stats(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-frontier",
   "metadata": {},
   "source": [
    "## DNN Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "mechanical-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 5\n",
    "lstm_size = 50\n",
    "weight_decay = 1e-2\n",
    "lr = 1e-4\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "straight-computer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 21.5933 - val_loss: 21.2238\n",
      "Epoch 2/200\n",
      "27/27 [==============================] - 1s 27ms/step - loss: 20.6290 - val_loss: 20.9625\n",
      "Epoch 3/200\n",
      "27/27 [==============================] - 1s 27ms/step - loss: 20.1106 - val_loss: 20.2305\n",
      "Epoch 4/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 19.6236 - val_loss: 19.8079\n",
      "Epoch 5/200\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 19.1183 - val_loss: 19.2879\n",
      "Epoch 6/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 18.6007 - val_loss: 18.7607\n",
      "Epoch 7/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 18.0780 - val_loss: 18.2126\n",
      "Epoch 8/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 17.5556 - val_loss: 17.6875\n",
      "Epoch 9/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 17.0372 - val_loss: 17.1602\n",
      "Epoch 10/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 16.5258 - val_loss: 16.6638\n",
      "Epoch 11/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 16.0231 - val_loss: 16.1505\n",
      "Epoch 12/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 15.5306 - val_loss: 15.6462\n",
      "Epoch 13/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 15.05 - 1s 35ms/step - loss: 15.0496 - val_loss: 15.1998\n",
      "Epoch 14/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 14.5805 - val_loss: 14.7395\n",
      "Epoch 15/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 14.1239 - val_loss: 14.2887\n",
      "Epoch 16/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 13.6800 - val_loss: 13.8342\n",
      "Epoch 17/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 13.2491 - val_loss: 13.4212\n",
      "Epoch 18/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 12.8307 - val_loss: 13.0028\n",
      "Epoch 19/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 12.4252 - val_loss: 12.6213\n",
      "Epoch 20/200\n",
      "27/27 [==============================] - 1s 26ms/step - loss: 12.0322 - val_loss: 12.2345\n",
      "Epoch 21/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 11.6516 - val_loss: 11.8600\n",
      "Epoch 22/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 11.2830 - val_loss: 11.5058\n",
      "Epoch 23/200\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 10.9262 - val_loss: 11.1649\n",
      "Epoch 24/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 10.5809 - val_loss: 10.8150\n",
      "Epoch 25/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 10.2467 - val_loss: 10.5003\n",
      "Epoch 26/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 9.9233 - val_loss: 10.1919\n",
      "Epoch 27/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 9.6103 - val_loss: 9.8856\n",
      "Epoch 28/200\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 9.3075 - val_loss: 9.5922\n",
      "Epoch 29/200\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 9.0144 - val_loss: 9.3092\n",
      "Epoch 30/200\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 8.7308 - val_loss: 9.0293\n",
      "Epoch 31/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 8.4563 - val_loss: 8.7631\n",
      "Epoch 32/200\n",
      "27/27 [==============================] - 1s 26ms/step - loss: 8.1907 - val_loss: 8.5113\n",
      "Epoch 33/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 7.9336 - val_loss: 8.2690\n",
      "Epoch 34/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 7.6848 - val_loss: 8.0217\n",
      "Epoch 35/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 7.4438 - val_loss: 7.7939\n",
      "Epoch 36/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 7.2106 - val_loss: 7.5683\n",
      "Epoch 37/200\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 6.9848 - val_loss: 7.3573\n",
      "Epoch 38/200\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 6.7661 - val_loss: 7.1508\n",
      "Epoch 39/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 6.5544 - val_loss: 6.9450\n",
      "Epoch 40/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 6.3493 - val_loss: 6.7487\n",
      "Epoch 41/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 6.1507 - val_loss: 6.5629\n",
      "Epoch 42/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 5.9583 - val_loss: 6.3779\n",
      "Epoch 43/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 5.7720 - val_loss: 6.2032\n",
      "Epoch 44/200\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 5.5915 - val_loss: 6.0295\n",
      "Epoch 45/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 5.4166 - val_loss: 5.8662\n",
      "Epoch 46/200\n",
      "27/27 [==============================] - 1s 27ms/step - loss: 5.2472 - val_loss: 5.7036\n",
      "Epoch 47/200\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 5.0831 - val_loss: 5.5514\n",
      "Epoch 48/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 4.9240 - val_loss: 5.4033\n",
      "Epoch 49/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 4.7699 - val_loss: 5.2620\n",
      "Epoch 50/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 4.6206 - val_loss: 5.1212\n",
      "Epoch 51/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 4.4759 - val_loss: 4.9883\n",
      "Epoch 52/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 4.3356 - val_loss: 4.8518\n",
      "Epoch 53/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 4.1998 - val_loss: 4.7344\n",
      "Epoch 54/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 4.0681 - val_loss: 4.6110\n",
      "Epoch 55/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 3.9404 - val_loss: 4.4841\n",
      "Epoch 56/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 3.8167 - val_loss: 4.3753\n",
      "Epoch 57/200\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 3.6969 - val_loss: 4.2612\n",
      "Epoch 58/200\n",
      "27/27 [==============================] - 1s 36ms/step - loss: 3.5807 - val_loss: 4.1561\n",
      "Epoch 59/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 3.4681 - val_loss: 4.0520\n",
      "Epoch 60/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 3.3590 - val_loss: 3.9482\n",
      "Epoch 61/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 3.2532 - val_loss: 3.8558\n",
      "Epoch 62/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 3.1507 - val_loss: 3.7583\n",
      "Epoch 63/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 3.0514 - val_loss: 3.6673\n",
      "Epoch 64/200\n",
      "27/27 [==============================] - 1s 36ms/step - loss: 2.9551 - val_loss: 3.5805\n",
      "Epoch 65/200\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 2.8618 - val_loss: 3.4912\n",
      "Epoch 66/200\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 2.7714 - val_loss: 3.4088\n",
      "Epoch 67/200\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 2.6837 - val_loss: 3.3260\n",
      "Epoch 68/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 2.5988 - val_loss: 3.2479\n",
      "Epoch 69/200\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 2.5165 - val_loss: 3.1764\n",
      "Epoch 70/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 2.4367 - val_loss: 3.0991\n",
      "Epoch 71/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 2.3594 - val_loss: 3.0302\n",
      "Epoch 72/200\n",
      "27/27 [==============================] - 1s 36ms/step - loss: 2.2844 - val_loss: 2.9575\n",
      "Epoch 73/200\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.2118 - val_loss: 2.8937\n",
      "Epoch 74/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 2.1414 - val_loss: 2.8250\n",
      "Epoch 75/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 2.0732 - val_loss: 2.7686\n",
      "Epoch 76/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 2.0072 - val_loss: 2.7041\n",
      "Epoch 77/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 1.9431 - val_loss: 2.6459\n",
      "Epoch 78/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 1.8811 - val_loss: 2.5868\n",
      "Epoch 79/200\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 1.8209 - val_loss: 2.5344\n",
      "Epoch 80/200\n",
      "27/27 [==============================] - 1s 36ms/step - loss: 1.7627 - val_loss: 2.4782\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 28ms/step - loss: 1.7062 - val_loss: 2.4276\n",
      "Epoch 82/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 1.6515 - val_loss: 2.3756\n",
      "Epoch 83/200\n",
      "27/27 [==============================] - 1s 27ms/step - loss: 1.5985 - val_loss: 2.3278\n",
      "Epoch 84/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 1.5472 - val_loss: 2.2846\n",
      "Epoch 85/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 1.4975 - val_loss: 2.2371\n",
      "Epoch 86/200\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 1.4493 - val_loss: 2.1921\n",
      "Epoch 87/200\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 1.4026 - val_loss: 2.1491\n",
      "Epoch 88/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 1.3574 - val_loss: 2.1088\n",
      "Epoch 89/200\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 1.3135 - val_loss: 2.0667\n",
      "Epoch 90/200\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 1.2711 - val_loss: 2.0274\n",
      "Epoch 91/200\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 1.2300 - val_loss: 1.9923\n",
      "Epoch 92/200\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 1.1902 - val_loss: 1.9576\n",
      "Epoch 93/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 1.1517 - val_loss: 1.9162\n",
      "Epoch 94/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 1.1143 - val_loss: 1.8916\n",
      "Epoch 95/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 1.0782 - val_loss: 1.8521\n",
      "Epoch 96/200\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 1.0431 - val_loss: 1.8231\n",
      "Epoch 97/200\n",
      "27/27 [==============================] - 1s 36ms/step - loss: 1.0092 - val_loss: 1.7894\n",
      "Epoch 98/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.9764 - val_loss: 1.7637\n",
      "Epoch 99/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.9446 - val_loss: 1.7402\n",
      "Epoch 100/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.9138 - val_loss: 1.7039\n",
      "Epoch 101/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.8841 - val_loss: 1.6778\n",
      "Epoch 102/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.8552 - val_loss: 1.6541\n",
      "Epoch 103/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.8273 - val_loss: 1.6310\n",
      "Epoch 104/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.8003 - val_loss: 1.6030\n",
      "Epoch 105/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.7741 - val_loss: 1.5818\n",
      "Epoch 106/200\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 0.7488 - val_loss: 1.5594\n",
      "Epoch 107/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.7243 - val_loss: 1.5377\n",
      "Epoch 108/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.7006 - val_loss: 1.5179\n",
      "Epoch 109/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 0.6777 - val_loss: 1.5025\n",
      "Epoch 110/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.6554 - val_loss: 1.4782\n",
      "Epoch 111/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.6340 - val_loss: 1.4649\n",
      "Epoch 112/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.6132 - val_loss: 1.4431\n",
      "Epoch 113/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.5931 - val_loss: 1.4276\n",
      "Epoch 114/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.5736 - val_loss: 1.4046\n",
      "Epoch 115/200\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 0.5548 - val_loss: 1.3976\n",
      "Epoch 116/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.5367 - val_loss: 1.3787\n",
      "Epoch 117/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.5191 - val_loss: 1.3681\n",
      "Epoch 118/200\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 0.5021 - val_loss: 1.3489\n",
      "Epoch 119/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.4856 - val_loss: 1.3337\n",
      "Epoch 120/200\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 0.4697 - val_loss: 1.3234\n",
      "Epoch 121/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.4544 - val_loss: 1.3172\n",
      "Epoch 122/200\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 0.4395 - val_loss: 1.3024\n",
      "Epoch 123/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.4251 - val_loss: 1.2915\n",
      "Epoch 124/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.4112 - val_loss: 1.2735\n",
      "Epoch 125/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.3978 - val_loss: 1.2692\n",
      "Epoch 126/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.3848 - val_loss: 1.2594\n",
      "Epoch 127/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.3723 - val_loss: 1.2436\n",
      "Epoch 128/200\n",
      "27/27 [==============================] - 1s 27ms/step - loss: 0.3602 - val_loss: 1.2406\n",
      "Epoch 129/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.3485 - val_loss: 1.2305\n",
      "Epoch 130/200\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 0.3372 - val_loss: 1.2222\n",
      "Epoch 131/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.3263 - val_loss: 1.2060\n",
      "Epoch 132/200\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 0.3158 - val_loss: 1.2007\n",
      "Epoch 133/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.3056 - val_loss: 1.2001\n",
      "Epoch 134/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.2958 - val_loss: 1.1902\n",
      "Epoch 135/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.2863 - val_loss: 1.1754\n",
      "Epoch 136/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.2771 - val_loss: 1.1689\n",
      "Epoch 137/200\n",
      "27/27 [==============================] - 1s 27ms/step - loss: 0.2683 - val_loss: 1.1799\n",
      "Epoch 138/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.2598 - val_loss: 1.1615\n",
      "Epoch 139/200\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 0.2515 - val_loss: 1.1565\n",
      "Epoch 140/200\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 0.2436 - val_loss: 1.1575\n",
      "Epoch 141/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.2360 - val_loss: 1.1539\n",
      "Epoch 142/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.2285 - val_loss: 1.1339\n",
      "Epoch 143/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.2214 - val_loss: 1.1361\n",
      "Epoch 144/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.2145 - val_loss: 1.1376\n",
      "Epoch 145/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.2079 - val_loss: 1.1238\n",
      "Epoch 146/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.2015 - val_loss: 1.1214\n",
      "Epoch 147/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.1953 - val_loss: 1.1254\n",
      "Epoch 148/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.1894 - val_loss: 1.1124\n",
      "Epoch 149/200\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 0.1837 - val_loss: 1.1086\n",
      "Epoch 150/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.1782 - val_loss: 1.1070\n",
      "Epoch 151/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.1728 - val_loss: 1.0932\n",
      "Epoch 152/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.1677 - val_loss: 1.1022\n",
      "Epoch 153/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.1628 - val_loss: 1.0925\n",
      "Epoch 154/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.1580 - val_loss: 1.0948\n",
      "Epoch 155/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.1534 - val_loss: 1.0962\n",
      "Epoch 156/200\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.1490 - val_loss: 1.0891\n",
      "Epoch 157/200\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 0.1448 - val_loss: 1.0788\n",
      "Epoch 158/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.1407 - val_loss: 1.0794\n",
      "Epoch 159/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.1368 - val_loss: 1.0848\n",
      "Epoch 160/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.1330 - val_loss: 1.0834\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 32ms/step - loss: 0.1293 - val_loss: 1.0760\n",
      "Epoch 162/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.1258 - val_loss: 1.0675\n",
      "Epoch 163/200\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 0.1224 - val_loss: 1.0805\n",
      "Epoch 164/200\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.1192 - val_loss: 1.0687\n",
      "Epoch 165/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.1161 - val_loss: 1.0775\n",
      "Epoch 166/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.1130 - val_loss: 1.0723\n",
      "Epoch 167/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.1102 - val_loss: 1.0719\n",
      "Epoch 168/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.1074 - val_loss: 1.0676\n",
      "Epoch 169/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.1047 - val_loss: 1.0667\n",
      "Epoch 170/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 0.1022 - val_loss: 1.0670\n",
      "Epoch 171/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.0997 - val_loss: 1.0710\n",
      "Epoch 172/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.0974 - val_loss: 1.0649\n",
      "Epoch 173/200\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.0951 - val_loss: 1.0579\n",
      "Epoch 174/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.0929 - val_loss: 1.0622\n",
      "Epoch 175/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.0908 - val_loss: 1.0553\n",
      "Epoch 176/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 0.0888 - val_loss: 1.0551\n",
      "Epoch 177/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.0869 - val_loss: 1.0645\n",
      "Epoch 178/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.0850 - val_loss: 1.0569\n",
      "Epoch 179/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.0833 - val_loss: 1.0535\n",
      "Epoch 180/200\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 0.0816 - val_loss: 1.0596\n",
      "Epoch 181/200\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 0.0799 - val_loss: 1.0593\n",
      "Epoch 182/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.0784 - val_loss: 1.0484\n",
      "Epoch 183/200\n",
      "27/27 [==============================] - 1s 26ms/step - loss: 0.0769 - val_loss: 1.0617\n",
      "Epoch 184/200\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 0.0755 - val_loss: 1.0518\n",
      "Epoch 185/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.0741 - val_loss: 1.0532\n",
      "Epoch 186/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.0728 - val_loss: 1.0517\n",
      "Epoch 187/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.0716 - val_loss: 1.0564\n",
      "Epoch 188/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.0703 - val_loss: 1.0530\n",
      "Epoch 189/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.0691 - val_loss: 1.0504\n",
      "Epoch 190/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.0680 - val_loss: 1.0575\n",
      "Epoch 191/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 0.0670 - val_loss: 1.0459\n",
      "Epoch 192/200\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 0.0659 - val_loss: 1.0554\n",
      "Epoch 193/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.0650 - val_loss: 1.0450\n",
      "Epoch 194/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.0640 - val_loss: 1.0521\n",
      "Epoch 195/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.0632 - val_loss: 1.0528\n",
      "Epoch 196/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.0624 - val_loss: 1.0538\n",
      "Epoch 197/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.0615 - val_loss: 1.0590\n",
      "Epoch 198/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.0607 - val_loss: 1.0514\n",
      "Epoch 199/200\n",
      "27/27 [==============================] - 1s 36ms/step - loss: 0.0600 - val_loss: 1.0520\n",
      "Epoch 200/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 0.0593 - val_loss: 1.0561\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Dense(1024, kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Dense(256, kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Dense(256, kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Dense(64, kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Dense(64, kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Dense(1, kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(opt, loss='mse')\n",
    "# fit model\n",
    "history = model.fit(X_train, y_train,\n",
    "          epochs=epochs,  batch_size=64, shuffle=True,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "floating-serbia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train_stats(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-politics",
   "metadata": {},
   "source": [
    "## LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "premium-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 5\n",
    "lstm_size = 200\n",
    "weight_decay = 1e-2\n",
    "lr = 1e-3\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "pregnant-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "trying-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_splited, y_train_splited = split_sequence(X_train, y_train, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "needed-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_splited, y_test_splited = split_sequence(X_test, y_test, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-daisy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "27/27 [==============================] - 3s 102ms/step - loss: 11.9273 - val_loss: 9.9481\n",
      "Epoch 2/200\n",
      "27/27 [==============================] - 2s 86ms/step - loss: 6.4157 - val_loss: 6.4695\n",
      "Epoch 3/200\n",
      "27/27 [==============================] - 2s 87ms/step - loss: 4.3251 - val_loss: 5.0559\n",
      "Epoch 4/200\n",
      "27/27 [==============================] - 2s 85ms/step - loss: 3.3733 - val_loss: 4.3026\n",
      "Epoch 5/200\n",
      "27/27 [==============================] - 2s 90ms/step - loss: 2.8094 - val_loss: 3.7996\n",
      "Epoch 6/200\n",
      "27/27 [==============================] - 3s 115ms/step - loss: 2.4237 - val_loss: 3.4144\n",
      "Epoch 7/200\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 2.1370 - val_loss: 3.1599\n",
      "Epoch 8/200\n",
      "15/27 [===============>..............] - ETA: 1s - loss: 1.9549"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024))\n",
    "model.add(LSTM(lstm_size, activation='relu', input_shape=(n_steps, 300), return_sequences=True,\n",
    "               kernel_regularizer=l2(weight_decay), recurrent_regularizer=l2(weight_decay),\n",
    "               bias_regularizer=l2(weight_decay)))\n",
    "model.add(Dense(256, kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Dense(16, kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "model.add(Dense(1, kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(opt, loss='mse')\n",
    "# fit model\n",
    "history = model.fit(X_train_splited, y_train_splited,\n",
    "          epochs=epochs,  batch_size=64, shuffle=True,\n",
    "          validation_data=(X_test_splited, y_test_splited))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-excuse",
   "metadata": {},
   "source": [
    "## Stacked LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "legal-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 5\n",
    "lstm_size = 200\n",
    "weight_decay = 1e-2\n",
    "lr = 1e-4\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "greater-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape = (20,50)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "vital-background",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "27/27 [==============================] - 1s 51ms/step - loss: 1.3438 - val_loss: 4.3225\n",
      "Epoch 2/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0988 - val_loss: 5.1925\n",
      "Epoch 3/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0426 - val_loss: 5.1918\n",
      "Epoch 4/200\n",
      "27/27 [==============================] - 1s 20ms/step - loss: 0.0384 - val_loss: 4.9664\n",
      "Epoch 5/200\n",
      "27/27 [==============================] - 1s 23ms/step - loss: 0.0305 - val_loss: 4.8114\n",
      "Epoch 6/200\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0240 - val_loss: 4.8989\n",
      "Epoch 7/200\n",
      "27/27 [==============================] - 1s 20ms/step - loss: 0.0200 - val_loss: 5.0878\n",
      "Epoch 8/200\n",
      "27/27 [==============================] - 1s 20ms/step - loss: 0.0156 - val_loss: 5.0386\n",
      "Epoch 9/200\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0141 - val_loss: 4.9850\n",
      "Epoch 10/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0142 - val_loss: 4.9261\n",
      "Epoch 11/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0128 - val_loss: 5.0639\n",
      "Epoch 12/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0127 - val_loss: 5.0006\n",
      "Epoch 13/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0125 - val_loss: 5.1039\n",
      "Epoch 14/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0113 - val_loss: 5.0047\n",
      "Epoch 15/200\n",
      "27/27 [==============================] - 0s 17ms/step - loss: 0.0110 - val_loss: 5.1471\n",
      "Epoch 16/200\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0104 - val_loss: 5.0257\n",
      "Epoch 17/200\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0102 - val_loss: 5.1629\n",
      "Epoch 18/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0110 - val_loss: 5.0074\n",
      "Epoch 19/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0106 - val_loss: 5.1211\n",
      "Epoch 20/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0104 - val_loss: 5.0379\n",
      "Epoch 21/200\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0103 - val_loss: 4.8936\n",
      "Epoch 22/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0098 - val_loss: 4.9596\n",
      "Epoch 23/200\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0095 - val_loss: 5.0031\n",
      "Epoch 24/200\n",
      "27/27 [==============================] - 1s 20ms/step - loss: 0.0092 - val_loss: 5.0616\n",
      "Epoch 25/200\n",
      "27/27 [==============================] - 1s 23ms/step - loss: 0.0087 - val_loss: 5.0944\n",
      "Epoch 26/200\n",
      "27/27 [==============================] - 1s 26ms/step - loss: 0.0095 - val_loss: 5.0243\n",
      "Epoch 27/200\n",
      "27/27 [==============================] - 1s 26ms/step - loss: 0.0091 - val_loss: 5.0276\n",
      "Epoch 28/200\n",
      "27/27 [==============================] - 1s 22ms/step - loss: 0.0090 - val_loss: 4.8823\n",
      "Epoch 29/200\n",
      "27/27 [==============================] - 1s 20ms/step - loss: 0.0094 - val_loss: 5.1136\n",
      "Epoch 30/200\n",
      "27/27 [==============================] - 1s 20ms/step - loss: 0.0089 - val_loss: 5.0457\n",
      "Epoch 31/200\n",
      "27/27 [==============================] - 1s 20ms/step - loss: 0.0086 - val_loss: 5.0097\n",
      "Epoch 32/200\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 0.0084 - val_loss: 5.0864\n",
      "Epoch 33/200\n",
      "27/27 [==============================] - 1s 25ms/step - loss: 0.0082 - val_loss: 5.0911\n",
      "Epoch 34/200\n",
      "27/27 [==============================] - 1s 24ms/step - loss: 0.0085 - val_loss: 5.0455\n",
      "Epoch 35/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0089 - val_loss: 4.9923\n",
      "Epoch 36/200\n",
      "27/27 [==============================] - 1s 27ms/step - loss: 0.0086 - val_loss: 5.0751\n",
      "Epoch 37/200\n",
      "27/27 [==============================] - 1s 23ms/step - loss: 0.0091 - val_loss: 4.9901\n",
      "Epoch 38/200\n",
      "27/27 [==============================] - 1s 23ms/step - loss: 0.0081 - val_loss: 5.0785\n",
      "Epoch 39/200\n",
      "27/27 [==============================] - 1s 20ms/step - loss: 0.0087 - val_loss: 4.9644\n",
      "Epoch 40/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.0091 - val_loss: 5.0249\n",
      "Epoch 41/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0087 - val_loss: 5.0053\n",
      "Epoch 42/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0083 - val_loss: 5.0626\n",
      "Epoch 43/200\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 0.0076 - val_loss: 5.0507\n",
      "Epoch 44/200\n",
      "27/27 [==============================] - 1s 23ms/step - loss: 0.0081 - val_loss: 5.0041\n",
      "Epoch 45/200\n",
      "27/27 [==============================] - 1s 24ms/step - loss: 0.0087 - val_loss: 5.0793\n",
      "Epoch 46/200\n",
      "27/27 [==============================] - 1s 25ms/step - loss: 0.0085 - val_loss: 5.0148\n",
      "Epoch 47/200\n",
      "27/27 [==============================] - 1s 25ms/step - loss: 0.0080 - val_loss: 5.0284\n",
      "Epoch 48/200\n",
      "27/27 [==============================] - 1s 20ms/step - loss: 0.0076 - val_loss: 5.0947\n",
      "Epoch 49/200\n",
      "27/27 [==============================] - 1s 24ms/step - loss: 0.0075 - val_loss: 5.0583\n",
      "Epoch 50/200\n",
      "27/27 [==============================] - 1s 23ms/step - loss: 0.0078 - val_loss: 5.0288\n",
      "Epoch 51/200\n",
      "27/27 [==============================] - 1s 20ms/step - loss: 0.0078 - val_loss: 5.0938\n",
      "Epoch 52/200\n",
      "27/27 [==============================] - 1s 23ms/step - loss: 0.0078 - val_loss: 5.0683\n",
      "Epoch 53/200\n",
      "27/27 [==============================] - 1s 24ms/step - loss: 0.0075 - val_loss: 5.0813\n",
      "Epoch 54/200\n",
      "27/27 [==============================] - 1s 22ms/step - loss: 0.0076 - val_loss: 4.9867\n",
      "Epoch 55/200\n",
      "27/27 [==============================] - 1s 27ms/step - loss: 0.0082 - val_loss: 5.0609\n",
      "Epoch 56/200\n",
      "27/27 [==============================] - 1s 26ms/step - loss: 0.0077 - val_loss: 5.0621\n",
      "Epoch 57/200\n",
      "27/27 [==============================] - 1s 25ms/step - loss: 0.0075 - val_loss: 5.1325\n",
      "Epoch 58/200\n",
      "27/27 [==============================] - 1s 27ms/step - loss: 0.0072 - val_loss: 5.0935\n",
      "Epoch 59/200\n",
      "27/27 [==============================] - 1s 22ms/step - loss: 0.0072 - val_loss: 5.0894\n",
      "Epoch 60/200\n",
      "27/27 [==============================] - 1s 23ms/step - loss: 0.0076 - val_loss: 5.0085\n",
      "Epoch 61/200\n",
      "27/27 [==============================] - 1s 25ms/step - loss: 0.0082 - val_loss: 5.0066\n",
      "Epoch 62/200\n",
      "27/27 [==============================] - 1s 25ms/step - loss: 0.0075 - val_loss: 5.1071\n",
      "Epoch 63/200\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 0.0074 - val_loss: 5.0468\n",
      "Epoch 64/200\n",
      "27/27 [==============================] - 1s 22ms/step - loss: 0.0069 - val_loss: 5.0613\n",
      "Epoch 65/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.0074 - val_loss: 4.9889\n",
      "Epoch 66/200\n",
      "27/27 [==============================] - 1s 23ms/step - loss: 0.0074 - val_loss: 5.1709\n",
      "Epoch 67/200\n",
      "27/27 [==============================] - 1s 22ms/step - loss: 0.0071 - val_loss: 5.0344\n",
      "Epoch 68/200\n",
      "27/27 [==============================] - 1s 24ms/step - loss: 0.0077 - val_loss: 5.0870\n",
      "Epoch 69/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0072 - val_loss: 5.2096\n",
      "Epoch 70/200\n",
      "27/27 [==============================] - 1s 25ms/step - loss: 0.0072 - val_loss: 4.9698\n",
      "Epoch 71/200\n",
      "27/27 [==============================] - 1s 22ms/step - loss: 0.0070 - val_loss: 5.0806\n",
      "Epoch 72/200\n",
      "27/27 [==============================] - 1s 23ms/step - loss: 0.0073 - val_loss: 5.0308\n",
      "Epoch 73/200\n",
      "27/27 [==============================] - 1s 24ms/step - loss: 0.0082 - val_loss: 5.1287\n",
      "Epoch 74/200\n",
      "27/27 [==============================] - 1s 22ms/step - loss: 0.0075 - val_loss: 5.0364\n",
      "Epoch 75/200\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0072 - val_loss: 5.0967\n",
      "Epoch 76/200\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0071 - val_loss: 5.0889\n",
      "Epoch 77/200\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0072 - val_loss: 5.0242\n",
      "Epoch 78/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0070 - val_loss: 5.1502\n",
      "Epoch 79/200\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0073 - val_loss: 4.9894\n",
      "Epoch 80/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0071 - val_loss: 5.1634\n",
      "Epoch 81/200\n",
      "27/27 [==============================] - 0s 17ms/step - loss: 0.0067 - val_loss: 5.0680\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0069 - val_loss: 4.9350\n",
      "Epoch 83/200\n",
      "27/27 [==============================] - 0s 17ms/step - loss: 0.0071 - val_loss: 5.0695\n",
      "Epoch 84/200\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0070 - val_loss: 5.0356\n",
      "Epoch 85/200\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 0.0071 - val_loss: 5.0805\n",
      "Epoch 86/200\n",
      "27/27 [==============================] - 1s 22ms/step - loss: 0.0073 - val_loss: 5.1835\n",
      "Epoch 87/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0073 - val_loss: 5.0808\n",
      "Epoch 88/200\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0073 - val_loss: 5.0595\n",
      "Epoch 89/200\n",
      "27/27 [==============================] - 1s 20ms/step - loss: 0.0072 - val_loss: 5.2006\n",
      "Epoch 90/200\n",
      "27/27 [==============================] - 0s 18ms/step - loss: 0.0071 - val_loss: 5.1526\n",
      "Epoch 91/200\n",
      "27/27 [==============================] - 1s 20ms/step - loss: 0.0068 - val_loss: 5.1136\n",
      "Epoch 92/200\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 0.0071 - val_loss: 5.0122\n",
      "Epoch 93/200\n",
      "27/27 [==============================] - 1s 24ms/step - loss: 0.0070 - val_loss: 5.1273\n",
      "Epoch 94/200\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 0.0071 - val_loss: 5.0985\n",
      "Epoch 95/200\n",
      "27/27 [==============================] - 1s 21ms/step - loss: 0.0067 - val_loss: 5.0701\n",
      "Epoch 96/200\n",
      "27/27 [==============================] - 1s 22ms/step - loss: 0.0069 - val_loss: 5.2163\n",
      "Epoch 97/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0070 - val_loss: 5.1287\n",
      "Epoch 98/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0067 - val_loss: 5.1112\n",
      "Epoch 99/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.0068 - val_loss: 5.1744\n",
      "Epoch 100/200\n",
      "27/27 [==============================] - 1s 29ms/step - loss: 0.0069 - val_loss: 5.0314\n",
      "Epoch 101/200\n",
      " 2/27 [=>............................] - ETA: 0s - loss: 0.0071"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-775ac487b4b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_train_splited, y_train_splited,\n\u001b[1;32m      2\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           validation_data=(X_test_splited, y_test_splited))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_splited, y_train_splited,\n",
    "          epochs=epochs,  batch_size=64, shuffle=True,\n",
    "          validation_data=(X_test_splited, y_test_splited))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_stats(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-company",
   "metadata": {},
   "source": [
    "## Conv LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "nervous-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Reshape((-1,1)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-count",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-poverty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
